{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ea6f796-d0eb-43b2-a567-bbea18c314a8",
   "metadata": {},
   "source": [
    "### To compare running the model on a CPU and GPU:\n",
    "\n",
    "- uses PyTorch's 'torch.device' to switch which device is being used\n",
    "- the CPU performs all computations for training (forward and backward passes) and inference\n",
    "\n",
    "## Plan:\n",
    "1. Model is defined by PyTorch's nn.Module\n",
    "2. Data is loaded using DataLoader\n",
    "3. Training: for epoch, model processes the training data by batch. For batch, data and target labels are moved to 'device', forward pass computes the output, loss is computed using 'CrossEntropyLoss', backward pass is performed to compute gradients, optimizer updates the model parameters\n",
    "4. Inference: model processes the test data by batch. For batch. data and target labels to moved to 'device', model performs forward pass to compute predictions. In addition, accuracy, inference time, throughput, CPU usage and memory usage are calculated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d18ae9-114d-49f4-b031-48d33961d311",
   "metadata": {},
   "source": [
    "# BEST Code to test both GPU and CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82ca035-b0ac-4ea7-9a8d-1e2e601861ae",
   "metadata": {},
   "source": [
    "### My GPU:\n",
    "Chipset Model: Apple M2\n",
    "- Type: GPU (Graphics Processing Unit)\n",
    "- Bus: Built-In\n",
    "- Total Number of Cores: 8\n",
    "\n",
    "### MY CPU:\n",
    "Model Name: MacBook Air\n",
    "- Model Identifier: Mac14,2\n",
    "- Chip: Apple M2\n",
    "- Total Number of Cores: 8 (4 performance and 4 efficiency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02e6c2f2-dd95-4052-bafa-fbb04710c3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import psutil\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "91ef3fcc-6e8a-4880-943a-6961cbf9466e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 586us/step - accuracy: 0.8637 - loss: 0.4710\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 578us/step - accuracy: 0.9554 - loss: 0.1488\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 588us/step - accuracy: 0.9671 - loss: 0.1093\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 612us/step - accuracy: 0.9719 - loss: 0.0900\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 584us/step - accuracy: 0.9769 - loss: 0.0726\n",
      "313/313 - 0s - 367us/step - accuracy: 0.9774 - loss: 0.0717\n",
      "CPU Training time: 6.02 seconds, Evaluation: [0.07167192548513412, 0.977400004863739]\n",
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.8597 - loss: 0.4802\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9550 - loss: 0.1527\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9660 - loss: 0.1164\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9730 - loss: 0.0892\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9762 - loss: 0.0747\n",
      "313/313 - 1s - 3ms/step - accuracy: 0.9776 - loss: 0.0731\n",
      "GPU Training time: 42.47 seconds, Evaluation: [0.07305588573217392, 0.9776000380516052]\n"
     ]
    }
   ],
   "source": [
    "#load and prepare the MNIST dataset\n",
    "#MNIST dataset contains handwritten digits from 0 to 9\n",
    "#x_train and x_test are the images, y_train and y_test are the corresponding labels\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "#normalize the pixel values to be between 0 and 1 (for better performance of the model)\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "\n",
    "#The purpose of this model is to classify handwritten digit images from the MNIST dataset into one of the 10 digit classes (0-9)\n",
    "\n",
    "#build the model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),  #flatten the 28x28 images into a 1D array of 784 elements\n",
    "    tf.keras.layers.Dense(128, activation='relu'),  \n",
    "    tf.keras.layers.Dropout(0.2),  #dropout layer to prevent overfitting by dropping 20% of the input units\n",
    "    tf.keras.layers.Dense(10)  #output layer with 10 neurons (one for each digit class)\n",
    "\n",
    "])\n",
    "\n",
    "#compile the model\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "#function to train and evaluate the model on a specific device\n",
    "def run_experiment(device_name):\n",
    "    with tf.device(device_name):\n",
    "        #rebuild the model for each device\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(10)\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\n",
    "        \n",
    "        #train the model\n",
    "        start_time = time.time()\n",
    "        model.fit(x_train, y_train, epochs=5) #train the model for 5 epochs on the training data\n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time #calculate the total training time\n",
    "        \n",
    "        #evaluate the model\n",
    "        evaluation = model.evaluate(x_test, y_test, verbose=2)\n",
    "        \n",
    "        return training_time, evaluation\n",
    "\n",
    "#run the experiment on CPU\n",
    "cpu_time, cpu_eval = run_experiment('/CPU:0')\n",
    "print(f\"CPU Training time: {cpu_time:.2f} seconds, Evaluation: {cpu_eval}\")\n",
    "\n",
    "#check if a GPU is available and run the experiment on GPU\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    gpu_time, gpu_eval = run_experiment('/GPU:0')\n",
    "    print(f\"GPU Training time: {gpu_time:.2f} seconds, Evaluation: {gpu_eval}\")\n",
    "else:\n",
    "    print(\"No GPU found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00e614f-3d85-4c34-8e98-922d53f81cb0",
   "metadata": {},
   "source": [
    "# Testing CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e408fc2-2b97-4da0-a165-38a4cbbb391d",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8247b135-cce3-4be6-9b1d-d13064245567",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import psutil\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728f1533-577e-4a6c-a384-459f69c486a2",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f1ecefec-7c95-476c-b157-3279a5a483e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define transformations to prepare dataset for training neural network\n",
    "#ToTensor - converts PIL Image/ Numpy Arrays into PyTorch tensor\n",
    "#Normalize - normalizes tensor images with mean and sd\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "#load datasets as train and test\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "#create dataloaders\n",
    "#load in 64 samples at a time\n",
    "#shuffled at every epoch to prevent learning unintended patterns/ overfitting\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92daf327-158a-4ce7-ae66-bb7118e44e3e",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a995c543-d081-467c-ba74-28a4af88f94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a simple neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    #initializes layers of the neural network\n",
    "    def __init__(self):\n",
    "        #constructor of parent class\n",
    "        super(SimpleNN, self).__init__()\n",
    "        #defines 3 linear (fully connected) layers\n",
    "        self.fc1 = nn.Linear(28 * 28, 128) #matches dimension size of input images, with 128 features in the layer\n",
    "        self.fc2 = nn.Linear(128, 64) \n",
    "        self.fc3 = nn.Linear(64, 10) #10 matches number of classification classes\n",
    "\n",
    "    #defines forward pass of the neural network\n",
    "    def forward(self, x):\n",
    "        #flattens input tensor\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        #should return classification class, a digit 0-9\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3521497d-bba1-40db-b588-c4a1eeb7052e",
   "metadata": {},
   "source": [
    "## System Metrics Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "49516efd-ff07-4dfb-ad87-2c9df7bb2c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get system metrics (cpu usage and memory)\n",
    "def get_system_metrics():\n",
    "    cpu_usage = psutil.cpu_percent()\n",
    "    memory_info = psutil.virtual_memory()\n",
    "    return cpu_usage, memory_info.percent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ca26de-c6e8-4ae4-9ccb-41dfca034d20",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0829945e-0ca1-4d33-b734-7effff61ca23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for training loop\n",
    "#set number of epoch to 5\n",
    "def train_model(model, train_loader, criterion, optimizer, device, num_epochs=5):\n",
    "    model.train() #sets model to training mode\n",
    "    total_training_time = 0  #initialize total training time\n",
    "    model.to(device)  # move model to the specified device\n",
    "    #loops over each 5 epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        #starts timer for time parameters\n",
    "        start_time = time.time()\n",
    "        epoch_loss = 0\n",
    "        #inner loop iterates over the batches of data from the training dataset\n",
    "        for batch_idx, (data, target) in enumerate(train_loader): \n",
    "            data, target = data.to(device), target.to(device)  # move data and target to the specified device\n",
    "            optimizer.zero_grad() #clears the gradients of optimized tensors\n",
    "            output = model(data) #passes training data through model\n",
    "            loss = criterion(output, target) #calculates loss (how well the model's predictions match the target values)\n",
    "            loss.backward()\n",
    "            optimizer.step() #updates model params\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        end_time = time.time()\n",
    "        total_epoch_time = end_time - start_time #calculates total time taken for epoch\n",
    "        total_training_time += total_epoch_time  # Accumulate total training time\n",
    "\n",
    "        cpu_usage, memory_usage = get_system_metrics()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_loader):.4f}, Time: {total_epoch_time:.2f}s, CPU Usage: {cpu_usage}%, Memory Usage: {memory_usage}%')\n",
    "\n",
    "    #print total training time after all epochs\n",
    "    print(f'Total Training Time: {total_training_time:.2f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2418a112-d7bc-459b-b727-fd5716b3f6a0",
   "metadata": {},
   "source": [
    "## Inference Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "61fff21d-051e-4fb7-98b3-704a977eef98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference loop\n",
    "#create function for evaluation, with model and test data as parameters\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval() #sets model to evaluation mode\n",
    "    model.to(device)  # move model to the specified device\n",
    "    #initialize metircs\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    total_inference_time = 0\n",
    "\n",
    "    with torch.no_grad(): #disables gradient calculation (reduces memory usage and speeds up)\n",
    "        for batch_idx, (data, target) in enumerate(test_loader): #loops through batches from the test dataset\n",
    "            data, target = data.to(device), target.to(device)  # move data and target to the specified device\n",
    "            start_time = time.time()\n",
    "            output = model(data)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            inference_time = end_time - start_time #calculates inference time for current batch\n",
    "            total_inference_time += inference_time #adds up each inference time\n",
    "            \n",
    "            _, predicted = torch.max(output.data, 1) #finds the class w highest predicted score for each sample in the batch\n",
    "            total_correct += (predicted == target).sum().item() #compares predicted with actual label, counts the total num of correct predictions\n",
    "            total_samples += target.size(0) #gets the number of samples in the current batch and adds to count of total samples processed\n",
    "\n",
    "    accuracy = total_correct / total_samples \n",
    "    avg_inference_time = total_inference_time / len(test_loader)\n",
    "    throughput = total_samples / total_inference_time #computes the num of samples processed per second\n",
    "    \n",
    "    cpu_usage, memory_usage = get_system_metrics() #uses function from above\n",
    "    \n",
    "    print(f'Accuracy: {accuracy:.4f}, Average Inference Time: {avg_inference_time:.4f}s, Throughput: {throughput:.2f} samples/s, CPU Usage: {cpu_usage}%, Memory Usage: {memory_usage}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e4af88-4157-4d7f-8647-c33c5f716c9b",
   "metadata": {},
   "source": [
    "## Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4bcfb80d-58b5-4af5-8e5a-edd39fb0f1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to run the entire workflow on a specified device\n",
    "def run_experiment(device):\n",
    "    #initialize the model\n",
    "    model = SimpleNN()\n",
    "    #define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss() #CEL measures how well the model's predictions match the actual labels, best for classification\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01) #SGD updates the model params \n",
    "\n",
    "    #train the model by running training loop\n",
    "    train_model(model, train_loader, criterion, optimizer, device, num_epochs=5)\n",
    "    #evaluate the model using function \n",
    "    evaluate_model(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "87f92bfe-b77a-4fff-80f0-9ce2ab24468b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu\n",
      "Epoch [1/5], Loss: 0.9828, Time: 2.18s, CPU Usage: 19.3%, Memory Usage: 84.1%\n",
      "Epoch [2/5], Loss: 0.3770, Time: 2.26s, CPU Usage: 77.0%, Memory Usage: 84.0%\n",
      "Epoch [3/5], Loss: 0.3218, Time: 2.19s, CPU Usage: 74.8%, Memory Usage: 83.7%\n",
      "Epoch [4/5], Loss: 0.2903, Time: 2.14s, CPU Usage: 79.8%, Memory Usage: 83.3%\n",
      "Epoch [5/5], Loss: 0.2659, Time: 2.14s, CPU Usage: 82.5%, Memory Usage: 83.2%\n",
      "Total Training Time: 10.90s\n",
      "Accuracy: 0.9277, Average Inference Time: 0.0001s, Throughput: 1092522.73 samples/s, CPU Usage: 75.4%, Memory Usage: 83.1%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Running on {device}\")\n",
    "\n",
    "\n",
    "#run the experiment on CPU\n",
    "run_experiment('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "09eda6a2-43af-4083-aefb-a16aeb93d0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #run the experiment on GPU (if available)\n",
    "# if torch.cuda.is_available():\n",
    "#     print(\"Running on GPU:\")\n",
    "#     run_experiment('cuda')\n",
    "# else:\n",
    "#     print(\"CUDA is not available. Skipping GPU run.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ec32f1-c6b9-400f-b4ea-1c70587b0d26",
   "metadata": {},
   "source": [
    "# Testing GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "37e7d25f-24d9-4b12-bcee-a215db9cc731",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import psutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cfd04741-ac82-4fe1-9aa9-8a8c41396818",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ddf5adb3-77c9-4b11-a4af-bc3fab9152e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install tensorflow-macos\n",
    "# !python -m pip install tensorflow-metal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2ec5a346-6eaa-4b41-8872-6675e3bcb46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.8539 - loss: 0.4920\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9551 - loss: 0.1556\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9684 - loss: 0.1062\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9741 - loss: 0.0845\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9767 - loss: 0.0744\n",
      "Training time: 49.17 seconds\n",
      "313/313 - 1s - 3ms/step - accuracy: 0.9762 - loss: 0.0735\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07352793961763382, 0.9762000441551208]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "# import time\n",
    "\n",
    "#load and prepare the MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "#build the model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "#compile the model\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "#train the model\n",
    "start_time = time.time()\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "#evaluate the model\n",
    "model.evaluate(x_test, y_test, verbose=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4554f47-922c-433e-a8bb-2bafe96e8f29",
   "metadata": {},
   "source": [
    "# BEST Code to test both GPU and CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3d2e4987-b944-4aa6-9a56-e5a58cb31acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 609us/step - accuracy: 0.8590 - loss: 0.4864\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 607us/step - accuracy: 0.9550 - loss: 0.1545\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 572us/step - accuracy: 0.9662 - loss: 0.1124\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 761us/step - accuracy: 0.9746 - loss: 0.0851\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 716us/step - accuracy: 0.9764 - loss: 0.0734\n",
      "313/313 - 0s - 449us/step - accuracy: 0.9791 - loss: 0.0700\n",
      "CPU Training time: 6.64 seconds, Evaluation: [0.06998570263385773, 0.9790999889373779]\n",
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.8614 - loss: 0.4788\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9545 - loss: 0.1563\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9667 - loss: 0.1097\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9721 - loss: 0.0876\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9785 - loss: 0.0714\n",
      "313/313 - 1s - 2ms/step - accuracy: 0.9762 - loss: 0.0758\n",
      "GPU Training time: 46.20 seconds, Evaluation: [0.07577118277549744, 0.9762000441551208]\n"
     ]
    }
   ],
   "source": [
    "#load and prepare the MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "#build the model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "#compile the model\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "#function to train and evaluate the model on a specific device\n",
    "def run_experiment(device_name):\n",
    "    with tf.device(device_name):\n",
    "        # Rebuild the model for each device\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(10)\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\n",
    "        \n",
    "        #train the model\n",
    "        start_time = time.time()\n",
    "        model.fit(x_train, y_train, epochs=5)\n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "        \n",
    "        #evaluate the model\n",
    "        evaluation = model.evaluate(x_test, y_test, verbose=2)\n",
    "        \n",
    "        return training_time, evaluation\n",
    "\n",
    "#run the experiment on CPU\n",
    "cpu_time, cpu_eval = run_experiment('/CPU:0')\n",
    "print(f\"CPU Training time: {cpu_time:.2f} seconds, Evaluation: {cpu_eval}\")\n",
    "\n",
    "#check if a GPU is available and run the experiment on GPU\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    gpu_time, gpu_eval = run_experiment('/GPU:0')\n",
    "    print(f\"GPU Training time: {gpu_time:.2f} seconds, Evaluation: {gpu_eval}\")\n",
    "else:\n",
    "    print(\"No GPU found\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
