{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ad0cf53-a28e-4d3f-9fca-e2e90db58cee",
   "metadata": {},
   "source": [
    "## Comparing Training and Evaluation Performance of a Question-Answering Model on CPU and GPU (MPS) using PyTorch\n",
    "- comparing the training and evaluation performance of a question-answering model using the CPU and GPU (MPS) on a Mac.\n",
    "- PyTorch's torch.device is used to switch between the CPU and GPU (MPS) for computation.\n",
    "\n",
    "\n",
    "## Plan:\n",
    "1. Model Definition: The model is defined using PyTorch's BertForQuestionAnswering from the transformers library.\n",
    "2. Data Loading: Data is loaded using a custom Dataset class and DataLoader.\n",
    "3. Training: For each epoch, the model processes the training data in batches. For each batch, data and target labels are moved to the specified device. The forward pass computes the output, the loss is computed using the model's loss function, the backward pass is performed to compute gradients, and the optimizer updates the model parameters.\n",
    "4. Inference: During inference, the model processes the test data in batches. For each batch, data and target labels are moved to the specified device, and the model performs a forward pass to compute predictions. Additionally, metrics such as accuracy, inference time, throughput, CPU usage, and memory usage are calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7d73cfb-a08e-43d5-bb2a-99bce7708373",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1603e8a6-f670-43ab-bf2e-68e26f8b595f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SQuAD dataset\n",
    "dataset = load_dataset('squad')\n",
    "\n",
    "# Use a smaller subset for testing\n",
    "small_train_dataset = dataset['train'].select(range(100))  # Select first 100 examples\n",
    "small_val_dataset = dataset['validation'].select(range(100))  # Select first 100 examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4df07fc-eaca-452f-8ff9-0462c9b50f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create class for Question Answering tasks\n",
    "class QADataset(Dataset):\n",
    "    #initializes the dataset w tokenizer, questions, contexts, answers and max length\n",
    "    def __init__(self, tokenizer, questions, contexts, answers, max_length=512):\n",
    "        self.tokenizer = tokenizer #to convert text to token ids\n",
    "        self.questions = questions #list of question strs\n",
    "        self.contexts = contexts #list of context strs\n",
    "        self.answers = answers #list of answer dictionaries\n",
    "        self.max_length = max_length\n",
    "\n",
    "    #method to get total number of samples in dataset\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    #method to retreive a single sample (question-context pair)\n",
    "    #returns a tuple containing the input ids, attention mask, start positions, and end positions\n",
    "    def __getitem__(self, idx):\n",
    "        #tokenize the pair at the given index\n",
    "        encoding = self.tokenizer(\n",
    "            self.questions[idx],  \n",
    "            self.contexts[idx], \n",
    "            truncation=True, \n",
    "            padding='max_length', \n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt' #returns pytorch tensors\n",
    "        )\n",
    "        \n",
    "        #extracts and squeezes the input_ids and attention_mask tensors to remove unnecessary dimensions\n",
    "        #ensures that the tensors are in the correct shape\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "\n",
    "        #convert the answer start and end position to PyTorch tensors\n",
    "        #ensures that the can be used in PyTorch's graph for gradient compuationa and backpropagation\n",
    "        start_positions = torch.tensor(self.answers[idx]['start'])\n",
    "        end_positions = torch.tensor(self.answers[idx]['end'])\n",
    "\n",
    "        #return as a tuple\n",
    "        return input_ids, attention_mask, start_positions, end_positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a662b3ee-33ba-4216-8dad-0d2a48b1b9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #initializes tokenizer from pre-trained 'bert-base-uncased' model\n",
    "# #converts text into token IDs for the model\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# #example data\n",
    "# questions = [\"What is AI?\", \"What is machine learning?\"]\n",
    "# contexts = [\"Artificial Intelligence is the simulation of human intelligence processes by machines.\", \n",
    "#             \"Machine learning is a subset of AI that focuses on the development of computer programs.\"]\n",
    "# answers = [{'start': 0, 'end': 2}, {'start': 0, 'end': 2}]\n",
    "\n",
    "# # #load the SQuAD dataset\n",
    "# # dataset = load_dataset('squad')\n",
    "\n",
    "# # #extract the list of questions from the training split of the dataset\n",
    "# # #dataset['train']] = list of dict, the list comprehension iterates over each example in the training split and extracts the value of the 'question' key\n",
    "# # questions = [example['question'] for example in dataset['train']]\n",
    "# # #extract the list of contexts (passages) from the training split of the dataset\n",
    "# # #the list comprehension iterates over each example in the training split and extracts the value of the 'context' key\n",
    "# # contexts = [example['context'] for example in dataset['train']]\n",
    "# # #extract the list of answers from the training split of the dataset\n",
    "# # answers = [{'start': example['answers']['answer_start'][0], 'end': example['answers']['answer_start'][0] + len(example['answers']['text'][0])} for example in dataset['train']]\n",
    "\n",
    "\n",
    "# #instantiates the QADataset with the tokenizer, questions, contexts, and answers\n",
    "# dataset = QADataset(tokenizer, questions, contexts, answers)\n",
    "# #create a DataLoader to load data from the QADataset, which will handle batching the data and shuffling it at every epoch\n",
    "# dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42fdef6c-3626-4b8d-9d97-e99ab72ca052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract questions, contexts, and answers from the smaller subset\n",
    "questions = [example['question'] for example in small_train_dataset]\n",
    "contexts = [example['context'] for example in small_train_dataset]\n",
    "answers = [{'start': example['answers']['answer_start'][0], 'end': example['answers']['answer_start'][0] + len(example['answers']['text'][0])} for example in small_train_dataset]\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "qa_dataset = QADataset(tokenizer, questions, contexts, answers)\n",
    "dataloader = DataLoader(qa_dataset, batch_size=8, shuffle=True, num_workers=4)  # Increase batch size and use 4 worker threads\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25cd6499-e87c-4d08-84bf-07def1a0b06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'GradScaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m BertForQuestionAnswering\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Use mixed precision training\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m scaler \u001b[38;5;241m=\u001b[39m GradScaler()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GradScaler' is not defined"
     ]
    }
   ],
   "source": [
    "#initialize BERT model \n",
    "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d14244d-5a7e-4b26-9d81-97b476732fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(device):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=3e-5)\n",
    "\n",
    "    # Training\n",
    "    start_time = time.time()\n",
    "    for epoch in range(3):  # Let's run for 3 epochs for simplicity\n",
    "        for batch in dataloader:\n",
    "            input_ids, attention_mask, start_positions, end_positions = [b.to(device) for b in batch]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=input_ids, \n",
    "                            attention_mask=attention_mask,\n",
    "                            start_positions=start_positions,\n",
    "                            end_positions=end_positions)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    inference_time = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids, attention_mask, start_positions, end_positions = [b.to(device) for b in batch]\n",
    "\n",
    "            start = time.time()\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            inference_time += time.time() - start\n",
    "\n",
    "            # Assuming simple accuracy for demonstration purposes\n",
    "            start_pred = torch.argmax(outputs.start_logits, dim=-1)\n",
    "            end_pred = torch.argmax(outputs.end_logits, dim=-1)\n",
    "\n",
    "            correct_predictions += ((start_pred == start_positions) & (end_pred == end_positions)).sum().item()\n",
    "            total_predictions += input_ids.size(0)\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return training_time, inference_time, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce58b441-c73f-4859-a4f8-f00c44434971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiment on CPU\n",
    "cpu_device = torch.device('cpu')\n",
    "cpu_accuracy, cpu_inference_time = train_and_evaluate(cpu_device)\n",
    "print(f\"CPU - Accuracy: {cpu_accuracy:.2f}, Inference Time: {cpu_inference_time:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafd4590-dc9f-4d45-a0c7-7041472c8a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if MPS is available and run on MPS\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device('mps')\n",
    "    mps_accuracy, mps_inference_time = train_and_evaluate(mps_device)\n",
    "    print(f\"MPS - Accuracy: {mps_accuracy:.2f}, Inference Time: {mps_inference_time:.2f}s\")\n",
    "else:\n",
    "    print(\"No MPS device found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16354ee-9c67-4952-9659-beada0b7bb7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b758493-fb74-42ce-8d0f-990d04c620af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0652d00-8ba3-4821-9a40-b28b58805d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'QADataset' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#run on CPU\u001b[39;00m\n\u001b[1;32m      2\u001b[0m cpu_device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m cpu_training_time, cpu_inference_time, cpu_accuracy \u001b[38;5;241m=\u001b[39m train_and_evaluate(cpu_device)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCPU - Training Time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcpu_training_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms, Inference Time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcpu_inference_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms, Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcpu_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[20], line 9\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m      7\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):  \u001b[38;5;66;03m# Let's run for 3 epochs for simplicity\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m     10\u001b[0m         input_ids, attention_mask, start_positions, end_positions \u001b[38;5;241m=\u001b[39m [b\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[1;32m     12\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:439\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_iterator()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:387\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1040\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1033\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1040\u001b[0m w\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Popen(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_context\u001b[38;5;241m.\u001b[39mget_context()\u001b[38;5;241m.\u001b[39mProcess\u001b[38;5;241m.\u001b[39m_Popen(process_obj)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/multiprocessing/context.py:288\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(process_obj)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_launch(process_obj)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinel \u001b[38;5;241m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(fp\u001b[38;5;241m.\u001b[39mgetbuffer())\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#run on CPU\n",
    "cpu_device = torch.device('cpu')\n",
    "cpu_training_time, cpu_inference_time, cpu_accuracy = train_and_evaluate(cpu_device)\n",
    "print(f\"CPU - Training Time: {cpu_training_time:.2f}s, Inference Time: {cpu_inference_time:.2f}s, Accuracy: {cpu_accuracy:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6c284dd1-c2d1-43c5-93ab-56b829d7af20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU - Training Time: 44.11s, Inference Time: 0.13s, Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "#check if GPU is available and run on MPS\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device('mps')\n",
    "    mps_training_time, mps_inference_time, mps_accuracy = train_and_evaluate(mps_device)\n",
    "    print(f\"GPU - Training Time: {mps_training_time:.2f}s, Inference Time: {mps_inference_time:.2f}s, Accuracy: {mps_accuracy:.2f}\")\n",
    "else:\n",
    "    print(\"No GPU device found\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
