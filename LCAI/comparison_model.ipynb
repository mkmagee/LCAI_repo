{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8247b135-cce3-4be6-9b1d-d13064245567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████| 9912422/9912422 [00:00<00:00, 11760833.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 28881/28881 [00:00<00:00, 795819.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 1648877/1648877 [00:00<00:00, 7130622.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 4542/4542 [00:00<00:00, 445772.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#import the necessary packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import psutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1ecefec-7c95-476c-b157-3279a5a483e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define transformations to prepare dataset for training neural network\n",
    "#ToTensor - converts PIL Image/ Numpy Arrays into PyTorch tensor\n",
    "#Normalize - normalizes tensor images with mean and sd\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "#load datasets as train and test\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "#create dataloaders\n",
    "#load in 64 samples at a time\n",
    "#shuffled at every epoch to prevent learning unintended patterns/ overfitting\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "#define a simple neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    #initializes layers of the neural network\n",
    "    def __init__(self):\n",
    "        #constructor of parent class\n",
    "        super(SimpleNN, self).__init__()\n",
    "        #defines 3 linear (fully connected) layers\n",
    "        self.fc1 = nn.Linear(28 * 28, 128) #matches dimension size of input images, with 128 features in the layer\n",
    "        self.fc2 = nn.Linear(128, 64) \n",
    "        self.fc3 = nn.Linear(64, 10) #10 matches number of classification classes\n",
    "\n",
    "    #defines forward pass of the neural network\n",
    "    def forward(self, x):\n",
    "        #flattens input tensor\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        #should return classification class, a digit 0-9\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49516efd-ff07-4dfb-ad87-2c9df7bb2c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get system metrics (cpu usage and memory)\n",
    "def get_system_metrics():\n",
    "    cpu_usage = psutil.cpu_percent()\n",
    "    memory_info = psutil.virtual_memory()\n",
    "    return cpu_usage, memory_info.percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0829945e-0ca1-4d33-b734-7effff61ca23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for training loop\n",
    "#set number of epoch to 5\n",
    "def train_model(model, train_loader, criterion, optimizer, device, num_epochs=5):\n",
    "    model.train() #sets model to training mode\n",
    "    total_training_time = 0  #initialize total training time\n",
    "    model.to(device)  # move model to the specified device\n",
    "    #loops over each 5 epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        #starts timer for time parameters\n",
    "        start_time = time.time()\n",
    "        epoch_loss = 0\n",
    "        #inner loop iterates over the batches of data from the training dataset\n",
    "        for batch_idx, (data, target) in enumerate(train_loader): \n",
    "            data, target = data.to(device), target.to(device)  # move data and target to the specified device\n",
    "            optimizer.zero_grad() #clears the gradients of optimized tensors\n",
    "            output = model(data) #passes training data through model\n",
    "            loss = criterion(output, target) #calculates loss (how well the model's predictions match the target values)\n",
    "            loss.backward()\n",
    "            optimizer.step() #updates model params\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        end_time = time.time()\n",
    "        total_epoch_time = end_time - start_time #calculates total time taken for epoch\n",
    "        total_training_time += total_epoch_time  # Accumulate total training time\n",
    "\n",
    "        cpu_usage, memory_usage = get_system_metrics()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_loader):.4f}, Time: {total_epoch_time:.2f}s, CPU Usage: {cpu_usage}%, Memory Usage: {memory_usage}%')\n",
    "\n",
    "    #print total training time after all epochs\n",
    "    print(f'Total Training Time: {total_training_time:.2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61fff21d-051e-4fb7-98b3-704a977eef98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference loop\n",
    "#create function for evaluation, with model and test data as parameters\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval() #sets model to evaluation mode\n",
    "    model.to(device)  # move model to the specified device\n",
    "    #initialize metircs\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    total_inference_time = 0\n",
    "\n",
    "    with torch.no_grad(): #disables gradient calculation (reduces memory usage and speeds up)\n",
    "        for batch_idx, (data, target) in enumerate(test_loader): #loops through batches from the test dataset\n",
    "            data, target = data.to(device), target.to(device)  # move data and target to the specified device\n",
    "            start_time = time.time()\n",
    "            output = model(data)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            inference_time = end_time - start_time #calculates inference time for current batch\n",
    "            total_inference_time += inference_time #adds up each inference time\n",
    "            \n",
    "            _, predicted = torch.max(output.data, 1) #finds the class w highest predicted score for each sample in the batch\n",
    "            total_correct += (predicted == target).sum().item() #compares predicted with actual label, counts the total num of correct predictions\n",
    "            total_samples += target.size(0) #gets the number of samples in the current batch and adds to count of total samples processed\n",
    "\n",
    "    accuracy = total_correct / total_samples \n",
    "    avg_inference_time = total_inference_time / len(test_loader)\n",
    "    throughput = total_samples / total_inference_time #computes the num of samples processed per second\n",
    "    \n",
    "    cpu_usage, memory_usage = get_system_metrics() #uses function from above\n",
    "    \n",
    "    print(f'Accuracy: {accuracy:.4f}, Average Inference Time: {avg_inference_time:.4f}s, Throughput: {throughput:.2f} samples/s, CPU Usage: {cpu_usage}%, Memory Usage: {memory_usage}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bcfb80d-58b5-4af5-8e5a-edd39fb0f1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to run the entire workflow on a specified device\n",
    "def run_experiment(device):\n",
    "    #initialize the model\n",
    "    model = SimpleNN()\n",
    "    #define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss() #CEL measures how well the model's predictions match the actual labels, best for classification\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01) #SGD updates the model params \n",
    "\n",
    "    #train the model by running training loop\n",
    "    train_model(model, train_loader, criterion, optimizer, device, num_epochs=5)\n",
    "    #evaluate the model using function \n",
    "    evaluate_model(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c6a17b4-9aa0-4da6-81c2-ee45612356c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU:\n",
      "Epoch [1/5], Loss: 1.0518, Time: 2.18s, CPU Usage: 15.8%, Memory Usage: 84.3%\n",
      "Epoch [2/5], Loss: 0.3828, Time: 2.13s, CPU Usage: 85.0%, Memory Usage: 84.3%\n",
      "Epoch [3/5], Loss: 0.3252, Time: 2.09s, CPU Usage: 86.9%, Memory Usage: 84.3%\n",
      "Epoch [4/5], Loss: 0.2941, Time: 2.11s, CPU Usage: 82.7%, Memory Usage: 84.3%\n",
      "Epoch [5/5], Loss: 0.2694, Time: 2.07s, CPU Usage: 81.2%, Memory Usage: 84.3%\n",
      "Total Training Time: 10.58s\n",
      "Accuracy: 0.9281, Average Inference Time: 0.0001s, Throughput: 956337.27 samples/s, CPU Usage: 82.4%, Memory Usage: 84.4%\n"
     ]
    }
   ],
   "source": [
    "#run the experiment on CPU\n",
    "print(\"Running on CPU:\")\n",
    "run_experiment('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09eda6a2-43af-4083-aefb-a16aeb93d0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available. Skipping GPU run.\n"
     ]
    }
   ],
   "source": [
    "#run the experiment on GPU (if available)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Running on GPU:\")\n",
    "    run_experiment('cuda')\n",
    "else:\n",
    "    print(\"CUDA is not available. Skipping GPU run.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
