{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ad0cf53-a28e-4d3f-9fca-e2e90db58cee",
   "metadata": {},
   "source": [
    "## Comparing Training and Evaluation Performance of a Question-Answering Model on CPU and GPU (MPS) using PyTorch\n",
    "- comparing the training and evaluation performance of a question-answering model using the CPU and GPU (MPS) on a Mac.\n",
    "- PyTorch's torch.device is used to switch between the CPU and GPU (MPS) for computation.\n",
    "\n",
    "\n",
    "## Plan:\n",
    "1. Model Definition: The model is defined using PyTorch's BertForQuestionAnswering from the transformers library.\n",
    "2. Data Loading: Data is loaded using a custom Dataset class and DataLoader.\n",
    "3. Training: For each epoch, the model processes the training data in batches. For each batch, data and target labels are moved to the specified device. The forward pass computes the output, the loss is computed using the model's loss function, the backward pass is performed to compute gradients, and the optimizer updates the model parameters.\n",
    "4. Inference: During inference, the model processes the test data in batches. For each batch, data and target labels are moved to the specified device, and the model performs a forward pass to compute predictions. Additionally, metrics such as accuracy, inference time, throughput, CPU usage, and memory usage are calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7d73cfb-a08e-43d5-bb2a-99bce7708373",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import time\n",
    "import psutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4df07fc-eaca-452f-8ff9-0462c9b50f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create class for Question Answering tasks\n",
    "class QADataset(Dataset):\n",
    "    #initializes the dataset w tokenizer, questions, contexts, answers and max length\n",
    "    def __init__(self, tokenizer, questions, contexts, answers, max_length=512):\n",
    "        self.tokenizer = tokenizer #to convert text to token ids\n",
    "        self.questions = questions #list of question strs\n",
    "        self.contexts = contexts #list of context strs\n",
    "        self.answers = answers #list of answer dictionaries\n",
    "        self.max_length = max_length\n",
    "\n",
    "    #method to get total number of samples in dataset\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    #method to retreive a single sample (question-context pair)\n",
    "    #returns a tuple containing the input ids, attention mask, start positions, and end positions\n",
    "    def __getitem__(self, idx):\n",
    "        #tokenize the pair at the given index\n",
    "        encoding = self.tokenizer(\n",
    "            self.questions[idx],  \n",
    "            self.contexts[idx], \n",
    "            truncation=True, \n",
    "            padding='max_length', \n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt' #returns pytorch tensors\n",
    "        )\n",
    "        \n",
    "        #extracts and squeezes the input_ids and attention_mask tensors to remove unnecessary dimensions\n",
    "        #ensures that the tensors are in the correct shape\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "\n",
    "        #convert the answer start and end position to PyTorch tensors\n",
    "        #ensures that the can be used in PyTorch's graph for gradient compuationa and backpropagation\n",
    "        start_positions = torch.tensor(self.answers[idx]['start'])\n",
    "        end_positions = torch.tensor(self.answers[idx]['end'])\n",
    "\n",
    "        #return as a tuple\n",
    "        return input_ids, attention_mask, start_positions, end_positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a662b3ee-33ba-4216-8dad-0d2a48b1b9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializes tokenizer from pre-trained 'bert-base-uncased' model\n",
    "#converts text into token IDs for the model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "#example data\n",
    "questions = [\"What is AI?\", \"What is machine learning?\"]\n",
    "contexts = [\"Artificial Intelligence is the simulation of human intelligence processes by machines.\", \n",
    "            \"Machine learning is a subset of AI that focuses on the development of computer programs.\"]\n",
    "answers = [{'start': 0, 'end': 2}, {'start': 0, 'end': 2}]\n",
    "\n",
    "# #load the SQuAD dataset\n",
    "# dataset = load_dataset('squad')\n",
    "\n",
    "# #extract the list of questions from the training split of the dataset\n",
    "# #dataset['train']] = list of dict, the list comprehension iterates over each example in the training split and extracts the value of the 'question' key\n",
    "# questions = [example['question'] for example in dataset['train']]\n",
    "# #extract the list of contexts (passages) from the training split of the dataset\n",
    "# #the list comprehension iterates over each example in the training split and extracts the value of the 'context' key\n",
    "# contexts = [example['context'] for example in dataset['train']]\n",
    "# #extract the list of answers from the training split of the dataset\n",
    "# answers = [{'start': example['answers']['answer_start'][0], 'end': example['answers']['answer_start'][0] + len(example['answers']['text'][0])} for example in dataset['train']]\n",
    "\n",
    "\n",
    "\n",
    "#instantiates the QADataset with the tokenizer, questions, contexts, and answers\n",
    "dataset = QADataset(tokenizer, questions, contexts, answers)\n",
    "#create a DataLoader to load data from the QADataset, which will handle batching the data and shuffling it at every epoch\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25cd6499-e87c-4d08-84bf-07def1a0b06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#initialize BERT model \n",
    "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4d14244d-5a7e-4b26-9d81-97b476732fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(device):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=3e-5)\n",
    "\n",
    "    # Training\n",
    "    start_time = time.time()\n",
    "    for epoch in range(3):  # Let's run for 3 epochs for simplicity\n",
    "        for batch in dataloader:\n",
    "            input_ids, attention_mask, start_positions, end_positions = [b.to(device) for b in batch]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=input_ids, \n",
    "                            attention_mask=attention_mask,\n",
    "                            start_positions=start_positions,\n",
    "                            end_positions=end_positions)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    inference_time = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids, attention_mask, start_positions, end_positions = [b.to(device) for b in batch]\n",
    "\n",
    "            start = time.time()\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            inference_time += time.time() - start\n",
    "\n",
    "            # Assuming simple accuracy for demonstration purposes\n",
    "            start_pred = torch.argmax(outputs.start_logits, dim=-1)\n",
    "            end_pred = torch.argmax(outputs.end_logits, dim=-1)\n",
    "\n",
    "            correct_predictions += ((start_pred == start_positions) & (end_pred == end_positions)).sum().item()\n",
    "            total_predictions += input_ids.size(0)\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return training_time, inference_time, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c0652d00-8ba3-4821-9a40-b28b58805d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU - Training Time: 15.33s, Inference Time: 0.46s, Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "#run on CPU\n",
    "cpu_device = torch.device('cpu')\n",
    "cpu_training_time, cpu_inference_time, cpu_accuracy = train_and_evaluate(cpu_device)\n",
    "print(f\"CPU - Training Time: {cpu_training_time:.2f}s, Inference Time: {cpu_inference_time:.2f}s, Accuracy: {cpu_accuracy:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6c284dd1-c2d1-43c5-93ab-56b829d7af20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU - Training Time: 44.11s, Inference Time: 0.13s, Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "#check if GPU is available and run on MPS\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device('mps')\n",
    "    mps_training_time, mps_inference_time, mps_accuracy = train_and_evaluate(mps_device)\n",
    "    print(f\"GPU - Training Time: {mps_training_time:.2f}s, Inference Time: {mps_inference_time:.2f}s, Accuracy: {mps_accuracy:.2f}\")\n",
    "else:\n",
    "    print(\"No GPU device found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38f5e028-74cf-405e-8cda-9ac817ad74f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU - Training Time: 26.42s, Inference Time: 0.38s, Accuracy: 1.00\n",
      "GPU - Training Time: 12.02s, Inference Time: 0.02s, Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Run on CPU\n",
    "cpu_device = torch.device('cpu')\n",
    "cpu_training_time, cpu_inference_time, cpu_accuracy = train_and_evaluate(cpu_device)\n",
    "print(f\"CPU - Training Time: {cpu_training_time:.2f}s, Inference Time: {cpu_inference_time:.2f}s, Accuracy: {cpu_accuracy:.2f}\")\n",
    "\n",
    "\n",
    "# Check if GPU is available and run on MPS\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device('mps')\n",
    "    mps_training_time, mps_inference_time, mps_accuracy = train_and_evaluate(mps_device)\n",
    "    print(f\"GPU - Training Time: {mps_training_time:.2f}s, Inference Time: {mps_inference_time:.2f}s, Accuracy: {mps_accuracy:.2f}\")\n",
    "else:\n",
    "    print(\"No GPU device found\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
